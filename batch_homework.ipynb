{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import udf\n",
    "from io import StringIO\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define URL\n",
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\"\n",
    "\n",
    "# Download CSV from URL\n",
    "response = requests.get(url)\n",
    "assert response.status_code == 200, 'Failed to download data'\n",
    "\n",
    "\n",
    "# Use pandas to read compressed CSV from URL\n",
    "data = pd.read_csv(url, compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 10:30:17 WARN TaskSetManager: Stage 0 contains a task of very large size (18148 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(6).write.mode(\"overwrite\").parquet(\"fhv.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 10:30:46 WARN TaskSetManager: Stage 3 contains a task of very large size (18148 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/03/02 10:30:50 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 14): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|       264.0|       264.0|    NaN|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|       264.0|       264.0|    NaN|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|       264.0|       264.0|    NaN|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|       264.0|       264.0|    NaN|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|       264.0|       264.0|    NaN|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 10:33:43 WARN TaskSetManager: Stage 7 contains a task of very large size (18148 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "filtered_df = df.filter(col('pickup_datetime').cast('date') == '2019-10-15')\n",
    "\n",
    "print(filtered_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 12:05:58 WARN TaskSetManager: Stage 11 contains a task of very large size (18148 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|     trip_duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|              B02832|2019-10-11 18:00:00|2091-10-11 18:30:00|       264.0|       264.0|    NaN|                B02832|          631152.5|\n",
      "|              B02832|2019-10-28 09:00:00|2091-10-28 09:30:00|       264.0|       264.0|    NaN|                B02832|          631152.5|\n",
      "|              B02416|2019-10-31 23:46:33|2029-11-01 00:13:00|         NaN|         NaN|    NaN|                B02416| 87672.44083333333|\n",
      "|     B00746         |2019-10-01 21:43:42|2027-10-01 21:45:23|       159.0|       264.0|    NaN|       B00746         | 70128.02805555555|\n",
      "|              B02921|2019-10-17 14:00:00|2020-10-18 00:00:00|         NaN|         NaN|    NaN|                B03037|            8794.0|\n",
      "|              B03110|2019-10-26 21:26:00|2020-10-26 21:36:00|       264.0|       264.0|    NaN|                B03110| 8784.166666666666|\n",
      "|              B03080|2019-10-30 12:30:04|2019-12-30 13:02:08|       264.0|        50.0|    NaN|                B02883|1464.5344444444445|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:54:33|       168.0|       235.0|    NaN|                B02765|1056.8266666666666|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:21:11|       168.0|       235.0|    NaN|                B02765|1056.2705555555556|\n",
      "|              B01452|2019-10-01 13:47:17|2019-11-03 15:20:28|        44.0|       214.0|    NaN|                B01452| 793.5530555555556|\n",
      "|              B01452|2019-10-01 07:21:12|2019-11-03 08:44:21|         5.0|         6.0|    NaN|                B01452| 793.3858333333334|\n",
      "|              B01452|2019-10-01 13:41:00|2019-11-03 14:58:51|       206.0|       172.0|    NaN|                B01452|          793.2975|\n",
      "|              B01452|2019-10-01 18:43:20|2019-11-03 19:43:13|        23.0|         5.0|    NaN|                B01452| 792.9980555555555|\n",
      "|              B01452|2019-10-01 18:43:46|2019-11-03 19:43:04|       251.0|        44.0|    NaN|                B01452| 792.9883333333333|\n",
      "|              B01452|2019-10-01 07:07:09|2019-11-03 07:58:46|       204.0|        23.0|    NaN|                B01452| 792.8602777777778|\n",
      "|              B01452|2019-10-01 14:49:28|2019-11-03 15:38:07|       214.0|       156.0|    NaN|                B01452| 792.8108333333333|\n",
      "|              B01452|2019-10-01 05:36:30|2019-11-03 06:23:36|       214.0|        84.0|    NaN|                B01452|           792.785|\n",
      "|              B00972|2019-10-01 15:02:55|2019-11-03 15:49:05|       204.0|         1.0|    NaN|                B00972| 792.7694444444444|\n",
      "|              B00972|2019-10-01 06:08:01|2019-11-03 06:53:15|       156.0|       204.0|    NaN|                B00972| 792.7538888888889|\n",
      "|              B01452|2019-10-01 06:41:17|2019-11-03 07:26:04|        44.0|        23.0|    NaN|                B01452| 792.7463888888889|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Calculate trip duration in seconds\n",
    "df = df.withColumn('trip_duration', \n",
    "                   (unix_timestamp('dropoff_datetime') - unix_timestamp('pickup_datetime'))/ 3600)\n",
    "\n",
    "# Fetch longest trips\n",
    "longest_trips = df.orderBy(col('trip_duration').desc())\n",
    "\n",
    "# Show the longest trips\n",
    "longest_trips.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 12:10:03 WARN TaskSetManager: Stage 21 contains a task of very large size (18148 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download CSV from URL\n",
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    "response = requests.get(url)\n",
    "assert response.status_code == 200, 'Failed to download data'\n",
    "\n",
    "# Use pandas to read CSV from URL\n",
    "data_zone = pd.read_csv(StringIO(response.content.decode('utf-8')))\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "df_zone = spark.createDataFrame(data_zone)\n",
    "\n",
    "# Create temporary tables\n",
    "df.createOrReplaceTempView(\"trips\")\n",
    "df_zone.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# SQL query to find out the least frequent pickup location\n",
    "query = \"\"\"\n",
    "SELECT zones.Zone, COUNT(trips.pickup_datetime) as count\n",
    "FROM trips\n",
    "JOIN zones ON trips.PULocationID = zones.LocationID\n",
    "GROUP BY zones.Zone\n",
    "ORDER BY count ASC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "least_frequent_pickup_location = spark.sql(query)\n",
    "\n",
    "# Show the result\n",
    "least_frequent_pickup_location.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
